---
title: "Observability (o11y)"
category: "Observability"
description: "Metrics, tracing, and logging for Spring Boot microservices"
---

# Mastery-Level Observability Curriculum for Spring Boot + Kotlin Microservices

Modern microservices architectures demand a fundamentally different approach to understanding system behavior—one that shifts from traditional monitoring ("is the system up?") to observability ("why is the system behaving this way?"). This 14-week curriculum provides a structured path from foundational concepts to production-ready expertise in observability for Spring Boot + Kotlin services deployed on AWS EKS, using an open-source stack comprising **OpenTelemetry**, **Micrometer**, **Prometheus**, **Grafana**, **Tempo**, and **Loki**.

The curriculum follows a depth-first mastery approach, where each phase builds upon the previous one with hands-on exercises, milestone projects, and clear success indicators. By completion, you will have both the theoretical foundation and practical skills to design, implement, and operate observability systems for production microservices.

---

## Phase 1: Foundations of observability (Weeks 1–2)

### Learning objectives

By the end of this phase, you should be able to articulate the fundamental differences between monitoring and observability, explain the three pillars and how they complement each other, and understand the unique challenges that microservices architectures present for understanding system behavior.

### Core concepts

**The three pillars of observability** form the foundation of modern system understanding. **Metrics** provide aggregated numerical measurements over time—they tell you *what* is happening at scale. **Logs** capture discrete events with rich contextual data—they tell you *why* something happened at a specific moment. **Traces** follow individual requests across service boundaries—they tell you *where* time was spent and how services interact.

The critical insight is that these pillars are not independent choices but complementary views of the same system. A metric spike indicates a problem exists; logs from that time window explain the error; traces show which services contributed to the failure and in what sequence.

**Traditional monitoring vs. observability** represents a philosophical shift. Traditional monitoring asks predefined questions: "Is CPU above 80%? Is the database responding?" This works when you know what can fail. Observability provides the capability to ask *arbitrary questions* about system state without deploying new instrumentation. In complex distributed systems, failures emerge from unexpected interactions—you cannot predict every failure mode in advance.

**Cardinality and dimensionality** are crucial concepts for metrics systems. Cardinality refers to the number of unique values a dimension can take. A `method` label with values GET, POST, PUT, DELETE has cardinality of 4. A `user_id` label with millions of users has extremely high cardinality. High cardinality explodes the number of time series Prometheus must track, consuming memory and degrading query performance. The rule of thumb: keep label cardinality under **10 unique values per label** and total time series in the tens of millions maximum.

**Context propagation** enables distributed tracing by passing trace identifiers across service boundaries. The **W3C Trace Context** standard (now default in Spring Boot 3.x) uses HTTP headers `traceparent` and `tracestate` to propagate a **128-bit trace ID** and **span ID** across services. When Service A calls Service B via HTTP, the trace context flows in headers; when Service B calls Service C via Kafka, the context embeds in message headers. This enables reconstructing the complete request path across dozens of services.

**Correlation IDs** tie together all telemetry for a single request. A trace ID serves as the primary correlation ID, appearing in metrics (via exemplars), logs (via MDC), and traces (naturally). This enables the "golden path": observe a latency spike in metrics → drill into the specific trace via exemplar → examine logs from all services involved in that trace.

### Hands-on exercises

1. **Telemetry taxonomy exercise**: Instrument a simple Spring Boot REST endpoint with manual logging, a counter metric, and a trace span. Write down exactly what information each captures and identify scenarios where each would be most useful for debugging.

2. **Cardinality calculation**: Given a Spring Boot service with metrics labeled by `method` (4 values), `uri` (20 endpoints), `status` (5 response code groups), and `instance` (3 pods), calculate total time series. Then calculate what happens if you add `user_id` with 100,000 users.

3. **Trace context inspection**: Use a simple two-service Spring Boot setup with RestTemplate. Enable debug logging and trace the W3C headers as they flow between services. Modify one service to drop the headers and observe the broken trace.

### Milestone project

Build a **"Telemetry Explainer" demonstration application**: three Spring Boot services (Order, Payment, Inventory) that call each other. Each service emits metrics, structured JSON logs, and traces. Create a document explaining exactly what telemetry is produced at each step of a sample request and how you would use each pillar to diagnose a simulated failure (add deliberate latency or error to Payment service).

### Essential resources

**Books:**
- *Distributed Systems Observability* by Cindy Sridharan — **FREE**, 36 pages, available at O'Reilly. Essential conceptual foundation.
- *Observability Engineering* by Charity Majors, Liz Fong-Jones, George Miranda (O'Reilly, 2nd edition 2024) — Chapters 1-5. The definitive text on observability philosophy.

**Documentation:**
- OpenTelemetry Concepts: https://opentelemetry.io/docs/concepts/
- W3C Trace Context specification: https://www.w3.org/TR/trace-context/

**Course:**
- Linux Foundation LFS148: "Getting Started with OpenTelemetry" — **FREE**, 10 hours. Complete this during Week 2.

### Success indicators

- [ ] Can explain the three pillars without notes and give examples of when each is most appropriate
- [ ] Can calculate time series cardinality given label dimensions
- [ ] Can trace W3C headers through a multi-service request manually
- [ ] Can articulate why observability matters more than monitoring for microservices

---

## Phase 2: What to measure — signals and metrics (Weeks 3–4)

### Learning objectives

Master the standard methodologies for deciding what to measure (Golden Signals, RED, USE), understand JVM and Spring Boot-specific metrics, learn to define business metrics and SLIs/SLOs, and develop intuition for which metrics indicate which problems.

### Core concepts

**The Four Golden Signals** from Google SRE provide a universal framework for service monitoring:

| Signal | Definition | Spring Boot Example |
|--------|-----------|---------------------|
| **Latency** | Time to serve a request (separate successful vs. failed) | `http_server_requests_seconds` histogram |
| **Traffic** | Demand on the system | `rate(http_server_requests_seconds_count[5m])` |
| **Errors** | Rate of failed requests | Requests with `status=5xx` / total requests |
| **Saturation** | How "full" the system is | Thread pool usage, connection pool exhaustion |

The key insight is that if you can only instrument four things, these four will catch most problems.

**RED method** (Rate, Errors, Duration) optimizes the Golden Signals for request-driven microservices. Every service should expose these three metrics for every endpoint. The Spring Boot Actuator with Micrometer provides these automatically via the `http.server.requests` metric with labels for `method`, `uri`, `status`, and `outcome`.

**USE method** (Utilization, Saturation, Errors) applies to infrastructure resources rather than services:

| Resource | Utilization | Saturation | Errors |
|----------|-------------|------------|--------|
| CPU | % time busy | Run queue length | — |
| Memory | % used | Swap activity, OOM events | — |
| Disk | % capacity used | I/O queue depth | Read/write errors |
| Thread Pool | Active / max threads | Rejected tasks | — |
| Connection Pool | Borrowed / max connections | Pending requests | Timeout errors |

**JVM-specific metrics** that Spring Boot + Micrometer expose automatically:

- **Heap memory**: `jvm_memory_used_bytes`, `jvm_memory_max_bytes` — Watch for heap pressure
- **GC behavior**: `jvm_gc_pause_seconds_sum`, `jvm_gc_pause_seconds_count` — Long GC pauses cause latency spikes
- **Thread pools**: `executor_active_threads`, `executor_queue_remaining` — Saturation indicators
- **HikariCP connection pool**: `hikaricp_connections_active`, `hikaricp_connections_pending` — Database saturation
- **Lettuce (Redis)**: `lettuce_command_completion_seconds` — Redis operation latency

**Spring Boot-specific metrics** include HTTP server request metrics, Spring WebFlux metrics for reactive services (observe backpressure via `reactor_netty_*` metrics), Spring Data repository timing, and Kafka consumer/producer metrics (`kafka_consumer_fetch_manager_records_consumed_total`, `kafka_producer_record_send_rate`).

**Business metrics and custom instrumentation** capture domain-specific KPIs that infrastructure metrics cannot: orders processed per minute, payment success rate, inventory turnover. These require explicit instrumentation using Micrometer's API.

**SLIs, SLOs, and error budgets** translate reliability into measurable commitments:

- **SLI (Service Level Indicator)**: A quantitative measure of service behavior, expressed as a ratio: `(Good events / Total events) × 100%`
- **SLO (Service Level Objective)**: Target value for an SLI over a time window: "99.9% of requests complete in under 200ms over 30 days"
- **Error Budget**: The inverse of the SLO target. A 99.9% SLO means 0.1% error budget—approximately 43 minutes of allowed downtime per month.

SLOs drive engineering decisions: when error budget is consumed, prioritize reliability over features. When budget is healthy, ship faster.

### Hands-on exercises

1. **Golden Signals dashboard**: Create a Grafana dashboard for a Spring Boot service showing all four Golden Signals. Use appropriate visualizations: heatmaps for latency distribution, gauges for saturation.

2. **JVM deep-dive**: Deploy a Spring Boot service under load (use k6 or wrk). Create a dashboard showing heap usage, GC pause times, thread pool utilization. Intentionally trigger a memory leak and identify it through metrics.

3. **SLO definition exercise**: For a hypothetical e-commerce checkout service, define SLIs and SLOs for availability and latency. Calculate the error budget for a 30-day window. Write PromQL queries that compute current SLI values.

4. **Custom business metrics**: Implement Micrometer counters, gauges, and timers for a domain scenario: order processing rate, cart abandonment rate, inventory levels. Use `@Timed` and `@Counted` annotations where appropriate.

### Milestone project

Create a **"Metrics Reference Application"** that demonstrates every category of metric discussed. Include:
- RED metrics for all HTTP endpoints (automatic via Actuator)
- USE metrics for thread pool, HikariCP connection pool, and JVM
- Custom business metrics for at least three domain KPIs
- An SLO configuration file documenting your SLI definitions and targets
- A Grafana dashboard bundle that visualizes all metrics with appropriate panel types

### Essential resources

**Books:**
- *Observability Engineering* Chapters 6-9 (SLOs, Alerting)
- Google SRE Book, Chapter 6: "Monitoring Distributed Systems" — FREE at sre.google

**Documentation:**
- Micrometer documentation: https://micrometer.io/docs
- Spring Boot Actuator metrics: https://docs.spring.io/spring-boot/reference/actuator/metrics.html

**Reference:**
- RED Method: https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture/
- USE Method: https://www.brendangregg.com/usemethod.html

### Success indicators

- [ ] Can implement Golden Signals monitoring for any Spring Boot service
- [ ] Can identify resource saturation from JVM and connection pool metrics
- [ ] Can define meaningful SLIs and calculate error budgets
- [ ] Can instrument custom business metrics using Micrometer API

---

## Phase 3: Instrumentation toolchain (Weeks 5–7)

### Learning objectives

Master the Spring Boot observability stack: Micrometer for metrics, OpenTelemetry for traces, structured logging with correlation. Understand auto-instrumentation versus manual instrumentation trade-offs and learn to implement end-to-end observability in production applications.

### Core concepts

**Micrometer as the metrics facade** abstracts metrics collection from the backend system. Spring Boot Actuator uses Micrometer to emit metrics that can export to Prometheus, Datadog, CloudWatch, or other systems by swapping the registry implementation. The `micrometer-registry-prometheus` dependency exposes a `/actuator/prometheus` endpoint with all metrics in Prometheus exposition format.

**Spring Boot 3.x Observation API** represents the future of Spring observability. The `io.micrometer:micrometer-observation` library provides a unified API that produces both metrics AND traces from a single instrumentation point. The `@Observed` annotation replaces separate `@Timed` for metrics and `@WithSpan` for traces.

**Micrometer Tracing** replaced Spring Cloud Sleuth starting with Spring Boot 3.0. Key migration details:
- Package change: `org.springframework.cloud.sleuth` → `io.micrometer.tracing`
- Properties: `spring.sleuth.*` → `management.tracing.*`
- Default trace format: 128-bit trace IDs (vs. 64-bit in Sleuth)
- Default propagation: W3C Trace Context (vs. B3 in Sleuth)

The `micrometer-tracing-bridge-otel` dependency connects Micrometer Tracing to OpenTelemetry as the backend.

**OpenTelemetry Java Agent** provides zero-code instrumentation for **150+ libraries** including:
- HTTP clients: Apache HttpClient, OkHttp, Java 11 HttpClient, WebClient, RestTemplate
- Databases: All JDBC, R2DBC, MongoDB, Cassandra, Redis (Lettuce, Jedis)
- Messaging: Kafka (producer/consumer/streams), RabbitMQ, JMS
- Frameworks: Spring MVC, Spring WebFlux, gRPC, Ktor

Running with the agent: `java -javaagent:opentelemetry-javaagent.jar -Dotel.service.name=my-service -jar app.jar`

The agent automatically propagates context across HTTP, Kafka, and async operations. For **Kotlin coroutines**, the agent includes dedicated instrumentation that handles context propagation across coroutine boundaries—this was historically problematic but is now well-supported.

**Manual instrumentation** adds custom spans and attributes where auto-instrumentation doesn't reach:

```kotlin
@WithSpan("process-order")
suspend fun processOrder(
    @SpanAttribute("order.id") orderId: String
): Order {
    // Automatically creates span with orderId attribute
}
```

**Structured logging with correlation** requires configuring Logback/SLF4J to include trace context in every log line. The OpenTelemetry Java agent automatically populates MDC with `trace_id` and `span_id`. Configure the log pattern:

```
%d{HH:mm:ss.SSS} %-5level [%X{trace_id},%X{span_id}] %logger{36} - %msg%n
```

Ship logs as JSON to enable Loki querying by trace ID:

```json
{"timestamp":"2026-02-02T10:15:30.123Z","level":"ERROR","trace_id":"abc123...","span_id":"def456...","message":"Payment failed","order_id":"ORD-789"}
```

**Exemplars** connect metrics to traces by attaching trace IDs to metric samples. When Micrometer records a histogram observation during an active trace, the trace ID becomes an exemplar on that metric. In Grafana, exemplars appear as clickable diamonds on metric graphs, linking directly to the corresponding trace.

Enable exemplars with:
```properties
management.metrics.distribution.percentiles-histogram.http.server.requests=true
```

**Context propagation edge cases**:
- **@Async methods**: Context is lost when spawning new threads. Use `ContextPropagatingTaskDecorator` to transfer trace context.
- **Project Reactor/WebFlux**: Use `.contextWrite()` to propagate observation context through reactive streams.
- **Kotlin coroutines**: The Java agent handles this; for manual propagation, use `opentelemetry-extension-kotlin` with `span.asContextElement()`.

### Week 5: Metrics instrumentation deep-dive

**Topics**: Micrometer registry configuration, custom meters (Counter, Gauge, Timer, DistributionSummary), histogram bucket configuration, metric naming conventions, @Timed/@Counted annotations, MeterFilter for conditional instrumentation.

**Exercise**: Implement comprehensive metrics for a shopping cart service including: request rate and latency by endpoint, cart item count distribution, checkout conversion funnel metrics, payment processor response times.

### Week 6: Distributed tracing implementation

**Topics**: OpenTelemetry Java agent configuration, auto-instrumented libraries, manual span creation, span attributes and events, baggage propagation, sampling configuration, W3C vs B3 propagation formats.

**Exercise**: Deploy three interconnected services (API Gateway → Order Service → Inventory Service) with the OTel agent. Verify traces flow correctly. Add manual spans for business logic (inventory reservation, payment processing). Test Kafka message tracing between services.

### Week 7: Logging and correlation

**Topics**: Structured JSON logging with Logback, MDC population, log shipping to Loki, LogQL fundamentals, exemplar configuration, building the correlation flow from metrics to traces to logs.

**Exercise**: Configure complete correlation: a user reports slow checkout. Using only Grafana, navigate from the p99 latency metric → exemplar → full distributed trace → relevant logs from each service in the trace. Document this investigation flow.

### Milestone project

Create a **"Fully Instrumented Microservices Demo"**:

Four Spring Boot + Kotlin services: `api-gateway`, `order-service`, `payment-service`, `inventory-service`. Communication via HTTP and Kafka. Requirements:
- OpenTelemetry Java agent on all services
- Custom spans for key business operations
- Structured JSON logs with trace correlation
- Prometheus metrics with exemplars enabled
- Custom business metrics (order rate, payment success rate, inventory levels)
- Documentation of the complete correlation flow

Deploy locally with Docker Compose including a local Grafana/Prometheus/Tempo/Loki stack.

### Essential resources

**Documentation:**
- OpenTelemetry Java instrumentation: https://opentelemetry.io/docs/languages/java/
- Spring Boot Observability: https://docs.spring.io/spring-boot/reference/actuator/observability.html
- Micrometer Tracing: https://micrometer.io/docs/tracing

**GitHub repositories:**
- `blueswen/spring-boot-observability` — Complete Spring Boot + LGTM stack demo with exemplars
- `mnadeem/boot-opentelemetry-tempo` — Multi-service demo with realistic scenarios
- `grafana/grafana-opentelemetry-starter` — Official Grafana Spring Boot starter

**Blog posts:**
- "Observability with Spring Boot 3" (spring.io official blog)
- "Distributed Request Tracing — Spring Boot 3, Micrometer Tracing with OpenTelemetry" (Medium)

### Success indicators

- [ ] Can configure OpenTelemetry Java agent with custom settings
- [ ] Can implement custom spans and attributes in Kotlin
- [ ] Can set up structured logging with trace correlation
- [ ] Can trace a request from metrics → exemplar → trace → logs
- [ ] Can explain context propagation across HTTP, Kafka, and async boundaries

---

## Phase 4: Infrastructure and platform setup (Weeks 8–10)

### Learning objectives

Deploy and configure the complete observability stack on Kubernetes: Prometheus Operator with ServiceMonitors, Grafana Loki for log aggregation, Grafana Tempo for trace storage, OpenTelemetry Collector for telemetry routing. Understand AWS EKS-specific considerations including IRSA and storage configurations.

### Core concepts

**kube-prometheus-stack** is the standard Helm chart for Kubernetes monitoring, bundling:
- Prometheus Operator (manages Prometheus via CRDs)
- Prometheus server with 15-day default retention
- Alertmanager for alert routing
- Grafana with pre-configured dashboards
- kube-state-metrics for Kubernetes object state
- node-exporter for host-level metrics

**Prometheus Operator CRDs** enable declarative monitoring configuration:

| CRD | Purpose |
|-----|---------|
| `ServiceMonitor` | Defines how to scrape Services (use for Spring Boot apps) |
| `PodMonitor` | Scrapes pods directly without Service |
| `PrometheusRule` | Defines alerting and recording rules |
| `AlertmanagerConfig` | Configures alert routing per namespace |

A ServiceMonitor for Spring Boot applications:
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: spring-boot-apps
  labels:
    release: prometheus  # Must match Helm release
spec:
  selector:
    matchLabels:
      app.kubernetes.io/framework: spring-boot
  endpoints:
    - port: http
      path: /actuator/prometheus
      interval: 30s
```

**Grafana Loki** provides horizontally scalable log aggregation without full-text indexing—it indexes only labels, storing log content in object storage (S3). Deployment modes:
- **Monolithic**: Single binary, up to ~20GB/day
- **Simple Scalable**: Read/write/backend separation, up to ~1TB/day
- **Microservices**: Full component separation for multi-TB/day

For EKS, deploy in Simple Scalable mode with S3 backend. Use **Grafana Alloy** (the successor to Promtail) as a DaemonSet to ship logs from all pods.

**Grafana Tempo** stores distributed traces in object storage. Unlike Jaeger (which requires Elasticsearch or Cassandra with full indexing), Tempo uses a trace ID-based lookup model that dramatically reduces storage costs. Traces are queryable via **TraceQL**, Tempo's query language.

Tempo vs. Jaeger decision matrix:
| Factor | Choose Tempo | Choose Jaeger |
|--------|--------------|---------------|
| Storage costs | Lower (object storage) | Higher (indexed) |
| Query capability | TraceQL, trace ID lookup | Full attribute search |
| Existing infra | Grafana stack | Elasticsearch/Cassandra already deployed |
| CNCF preference | Grafana Labs project | CNCF Graduated |

**OpenTelemetry Collector** serves as the telemetry pipeline hub, receiving OTLP data from applications and routing to backends. Deployment patterns:

| Pattern | Kubernetes Mode | Use Case |
|---------|-----------------|----------|
| Agent | DaemonSet | Per-node collection, host metadata |
| Gateway | Deployment | Centralized processing, tail sampling |
| Sidecar | Per-pod | Application-specific, Fargate environments |

The recommended production pattern is **layered**: DaemonSet agents collect and forward; Deployment gateways handle heavy processing (tail sampling, batching, filtering).

**EKS-specific configuration**:

1. **IRSA (IAM Roles for Service Accounts)**: Grant Loki and Tempo pods S3 access without static credentials
2. **Storage classes**: Use `gp3` for Prometheus PersistentVolumes (IOPS/throughput configurable)
3. **EBS CSI driver**: Required for dynamic PV provisioning
4. **AWS managed alternatives**: Amazon Managed Prometheus (AMP) and Amazon Managed Grafana (AMG) reduce operational burden but limit customization

### Week 8: Prometheus and Grafana deployment

**Topics**: Helm chart configuration, kube-prometheus-stack values customization, ServiceMonitor creation, Prometheus storage sizing, Grafana provisioning (dashboards and datasources as code).

**Exercise**: Deploy kube-prometheus-stack on a local kind/minikube cluster or EKS. Create ServiceMonitors for Spring Boot applications. Configure 50GB storage with 15-day retention. Provision a custom dashboard via ConfigMap.

### Week 9: Loki and Tempo deployment

**Topics**: Loki architecture (Simple Scalable mode), S3 backend configuration, Grafana Alloy for log shipping, Tempo deployment with S3 storage, datasource configuration in Grafana, LogQL and TraceQL basics.

**Exercise**: Deploy Loki and Tempo via Helm with S3 backends (use MinIO locally or real S3 on EKS). Configure Grafana Alloy to ship logs. Verify logs appear in Grafana Explore. Deploy a traced application and query traces in Tempo.

### Week 10: OpenTelemetry Collector and integration

**Topics**: Collector architecture (receivers, processors, exporters, pipelines), configuration language, DaemonSet vs Deployment modes, k8sattributes processor for pod metadata, memory limiter processor, batch processor tuning.

**Exercise**: Deploy OpenTelemetry Collector as a Gateway. Configure it to receive OTLP from applications and export to Prometheus (metrics), Loki (logs), and Tempo (traces). Add k8sattributes processor to enrich telemetry with pod/namespace labels. Test the full pipeline.

### Milestone project

**Production-Ready EKS Observability Stack**:

Deploy on AWS EKS (use eksctl for cluster creation):
- kube-prometheus-stack with 100GB storage, ServiceMonitors for all namespaces
- Grafana Loki in Simple Scalable mode with S3 backend, IRSA configured
- Grafana Tempo with S3 backend
- OpenTelemetry Collector as DaemonSet + Deployment (layered pattern)
- Complete Grafana configuration: all datasources, trace-to-logs correlation, exemplar configuration
- Terraform/Helm values files for reproducible deployment

Document the architecture with a diagram showing data flow from applications through Collector to storage backends.

### Essential resources

**Helm charts:**
- `prometheus-community/kube-prometheus-stack`: https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack
- `grafana/loki`: https://artifacthub.io/packages/helm/grafana/loki
- `grafana/tempo`: https://artifacthub.io/packages/helm/grafana/tempo
- `open-telemetry/opentelemetry-collector`: https://artifacthub.io/packages/helm/opentelemetry-helm/opentelemetry-collector

**Documentation:**
- Prometheus Operator: https://prometheus-operator.dev/docs/prologue/introduction/
- Grafana Loki: https://grafana.com/docs/loki/latest/
- Grafana Tempo: https://grafana.com/docs/tempo/latest/
- OpenTelemetry Collector: https://opentelemetry.io/docs/collector/

**Tutorial:**
- DigitalOcean Kubernetes Starter Kit observability chapter: https://github.com/digitalocean/Kubernetes-Starter-Kit-Developers

### Success indicators

- [ ] Can deploy and configure kube-prometheus-stack from scratch
- [ ] Can create ServiceMonitors for custom applications
- [ ] Can deploy Loki/Tempo with S3 backends on EKS
- [ ] Can configure OpenTelemetry Collector pipelines
- [ ] Can troubleshoot missing metrics/logs/traces in the pipeline

---

## Phase 5: Making metrics accessible (Weeks 11–12)

### Learning objectives

Design effective dashboards for different audiences, implement SLO-based alerting with multi-burn-rate windows, configure Alertmanager routing and silencing, establish on-call practices, and create runbooks linked to alerts.

### Core concepts

**Dashboard design principles**:

1. **Tell a story**: Dashboards should guide users from overview to detail. Start with aggregate health, drill into service-specific views, then detailed debugging panels.

2. **Dashboard hierarchy**:
   - Level 1: Overview (system health, SLO status, critical alerts)
   - Level 2: Service dashboards (RED metrics per service)
   - Level 3: Detailed/debug dashboards (per-instance, traces, logs)

3. **Audience-specific dashboards**:
   | Audience | Focus | Refresh Rate |
   |----------|-------|--------------|
   | Engineering | Detailed metrics, debugging | 5-30 seconds |
   | Operations/SRE | SLO status, incidents | 1-5 minutes |
   | Management | Business KPIs, trends | Hourly/daily |

4. **Visualization best practices**:
   - Use **heatmaps** for latency distributions (shows patterns time series can't)
   - Use **stat panels** for current values (error rate now)
   - Use **time series** for trends over time
   - Avoid pie charts for changing data
   - Use consistent color schemes (red=bad is universal)

**Multi-window, multi-burn-rate alerting** is the Google SRE-recommended approach. Instead of alerting on a simple threshold, compare error rate against the sustainable rate that would exhaust the error budget exactly at window end.

**Burn rate** = (observed error rate) / (SLO error budget)
- Burn rate 1 = consuming budget at exactly the sustainable rate
- Burn rate 14.4 = will exhaust 30-day budget in 2 hours

The dual-window approach requires both a long window AND a short window to exceed thresholds, filtering transient spikes:

| Severity | Long Window | Short Window | Burn Rate | Page? |
|----------|-------------|--------------|-----------|-------|
| Critical | 1 hour | 5 minutes | 14.4 | Yes |
| High | 6 hours | 30 minutes | 6 | Yes |
| Medium | 3 days | 6 hours | 1 | Ticket |

**Alertmanager configuration** handles routing, grouping, and silencing:
- **Routing**: Direct critical alerts to PagerDuty, warnings to Slack
- **Grouping**: Combine related alerts to avoid notification flood
- **Inhibition**: Suppress child alerts when parent fires (e.g., suppress all pod alerts when node is down)
- **Silencing**: Temporarily mute alerts during maintenance

**SLO management tools**:
- **Sloth** (github.com/slok/sloth): Generates Prometheus recording rules and multi-burn-rate alerts from SLO definitions
- **Pyrra** (github.com/pyrra-dev/pyrra): Kubernetes CRD-based SLO management with web UI

**Runbooks** should accompany every actionable alert:
1. What is this alert? (Clear description)
2. Impact (Who/what is affected)
3. Triage steps (Decision tree)
4. Remediation (Copy-pasteable commands)
5. Escalation (Who to contact if steps fail)
6. Related dashboards (Links to Grafana)

### Week 11: Dashboard design and creation

**Topics**: Grafana dashboard best practices, variable templates, panel types and when to use each, dashboard JSON provisioning, creating RED/USE dashboards, business KPI dashboards.

**Exercise**: Create a three-tier dashboard hierarchy for the microservices demo:
1. Overview dashboard: System health summary, SLO status gauges, links to service dashboards
2. Service dashboard for Order Service: RED metrics, dependency health, recent traces
3. Debug dashboard: Per-pod metrics, log panel, trace exploration

### Week 12: Alerting and incident response

**Topics**: PromQL for alerting, multi-burn-rate alert rules, Alertmanager routing configuration, Grafana alerting for logs, SLO management with Sloth, runbook templates.

**Exercise**: Implement complete alerting for the Order Service:
1. Define SLOs: 99.9% availability, 99% requests under 200ms
2. Generate multi-burn-rate alerts using Sloth
3. Configure Alertmanager to route critical alerts to a test webhook (simulating PagerDuty)
4. Write runbooks for each alert
5. Test alerts by injecting failures

### Milestone project

**Complete Alerting and Dashboard Package**:

Deliverables:
- SLO definition document for all services (SLI formulas, targets, error budgets)
- Sloth configuration generating all alert rules
- Alertmanager configuration with routing for critical/warning/info severities
- Dashboard bundle: overview, per-service, debug dashboards
- Runbook documentation for all critical alerts
- Incident response playbook documenting the investigation flow

### Essential resources

**Books:**
- Google SRE Workbook, Chapter 5: "Alerting on SLOs" — FREE at sre.google/workbook

**Documentation:**
- Grafana dashboard best practices: https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/best-practices/
- Alertmanager configuration: https://prometheus.io/docs/alerting/latest/configuration/
- Sloth documentation: https://sloth.dev/

**Course:**
- Grafana Workshops (free): https://grafana.com/workshops/

**Tools:**
- Sloth: https://github.com/slok/sloth
- Pyrra: https://github.com/pyrra-dev/pyrra

### Success indicators

- [ ] Can design dashboards for different audiences
- [ ] Can implement multi-burn-rate alerting from SLO definitions
- [ ] Can configure Alertmanager routing and silencing
- [ ] Can create linked runbooks for alerts
- [ ] Can explain error budget policy and how it drives engineering decisions

---

## Phase 6: Advanced topics and mastery (Weeks 13–14)

### Learning objectives

Handle production challenges: high-cardinality metrics, trace sampling at scale, correlation workflows, performance tuning, observability-driven development, and chaos engineering integration.

### Core concepts

**High-cardinality metrics management**:

Prometheus struggles when time series count explodes. Causes include unbounded labels (user IDs, request IDs, UUIDs) and label multiplication effects.

Strategies:
1. **Drop high-cardinality labels** via metric_relabel_configs
2. **Use recording rules** to pre-aggregate
3. **Increase scrape interval** for less critical metrics (60s vs 15s = 75% reduction)
4. **Use logs instead** for user-level granularity

Rule of thumb: If a label can take >100 unique values, it probably belongs in logs, not metrics.

**Trace sampling strategies**:

| Type | Decision Point | Best For |
|------|----------------|----------|
| Head-based | At trace start | Simple, low overhead |
| Tail-based | After trace completion | Error capture, slow request capture |

**Tail-based sampling** with OpenTelemetry Collector enables intelligent decisions—sample 100% of errors and slow requests, but only 1% of normal traffic:

```yaml
processors:
  tail_sampling:
    policies:
      - name: errors-always
        type: status_code
        status_code: {status_codes: [ERROR]}
      - name: slow-requests
        type: latency
        latency: {threshold_ms: 2000}
      - name: probabilistic
        type: probabilistic
        probabilistic: {sampling_percentage: 1}
```

**Production correlation workflow**:

1. User reports: "Checkout is slow"
2. Check p99 latency dashboard → see spike at 14:30
3. Click exemplar on the spike → opens trace in Tempo
4. Trace shows Payment Service took 3.2 seconds
5. Click "View logs" in Tempo → Loki shows logs from all services in trace
6. Logs show: "External payment gateway timeout"

This workflow requires: exemplars enabled, Tempo trace-to-logs configured, logs containing trace_id.

**Performance impact of instrumentation**:

| Component | Typical Overhead | Mitigation |
|-----------|------------------|------------|
| OTel Java Agent | +1-2s startup, <5% CPU | Disable unused instrumentations |
| High sampling | 10-30% throughput reduction | Sample aggressively (1-10%) |
| Synchronous logging | Significant latency | Use async appenders |
| High-cardinality metrics | Memory pressure | Drop/aggregate labels |

**Observability-driven development** integrates observability into the development lifecycle:
- Include observability in Definition of Done
- Run local observability stack (docker-lgtm image provides single-container LGTM)
- Write tests that assert metrics are emitted
- Track CI/CD pipeline observability (DORA metrics)

**Chaos engineering** uses observability to validate resilience:
1. Define steady state using current metrics (baseline error rate, p99 latency)
2. Hypothesize what will happen during failure
3. Inject failure (pod kill, network latency, CPU stress)
4. Monitor metrics/traces/logs for deviation from steady state
5. Halt if thresholds breached
6. Analyze and improve

Tools: LitmusChaos (Kubernetes-native), Gremlin (commercial), Chaos Mesh (CNCF sandbox)

### Week 13: Production challenges

**Topics**: High-cardinality metrics identification and remediation, tail-based sampling configuration, performance profiling of instrumentation overhead, advanced correlation configuration (trace-to-logs, trace-to-metrics).

**Exercise**: 
1. Deploy an application that generates high-cardinality metrics (add user_id label). Observe Prometheus memory growth. Implement relabeling to drop the label.
2. Configure OpenTelemetry Collector with tail-based sampling: 100% errors, 100% slow requests (>1s), 5% of remaining traffic.
3. Benchmark a service with and without OpenTelemetry agent; measure throughput difference.

### Week 14: Advanced practices

**Topics**: Observability-driven development, CI/CD observability, chaos engineering integration, organizational adoption patterns, observability maturity model.

**Exercise**:
1. Add observability to a CI/CD pipeline: track build times, test pass rates as metrics
2. Run a chaos experiment: kill a pod during load test, verify you can detect and diagnose the failure using only observability tools
3. Create an "observability onboarding guide" for a new team member

### Milestone project

**Capstone: Production Observability Platform**

Comprehensive demonstration of mastery:

1. **Architecture document**: Complete observability architecture for a microservices platform, including data flow diagrams, component choices with rationale, scaling considerations

2. **Implementation**: Full stack deployed on EKS with all advanced features:
   - Tail-based sampling via Collector
   - Complete correlation (metrics ↔ traces ↔ logs)
   - SLO-based alerting with burn-rate windows
   - High-cardinality handling strategy

3. **Runbook library**: Complete runbooks for all alert scenarios

4. **Chaos experiment**: Document a chaos engineering experiment, including hypothesis, procedure, observability-based validation, and findings

5. **Cost analysis**: Estimate storage and compute costs for the observability platform at projected scale (1000 services, 100k requests/second)

### Essential resources

**Books:**
- *Cloud-Native Observability with OpenTelemetry* by Alex Boten (Packt) — Advanced OTel patterns
- *Mastering OpenTelemetry and Observability* by Steve Flanders (Wiley, 2024) — Enterprise adoption, eBPF

**Documentation:**
- OpenTelemetry sampling: https://opentelemetry.io/docs/concepts/sampling/
- Tail sampling processor: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor

**Certification:**
- OpenTelemetry Certified Associate (OTCA) — CNCF certification, $250

**Advanced resources:**
- KubeCon Observability Day talks (recorded sessions on YouTube)
- Grafana Labs blog for production patterns

### Success indicators

- [ ] Can diagnose and remediate high-cardinality metrics issues
- [ ] Can configure tail-based sampling for production workloads
- [ ] Can execute complete correlation workflow from alert to root cause
- [ ] Can quantify and optimize instrumentation overhead
- [ ] Can design and run chaos experiments validated by observability
- [ ] Can create organizational observability adoption strategy

---

## Comprehensive resource index

### Books (ordered by importance)

| Book | Author(s) | Publisher | Focus |
|------|-----------|-----------|-------|
| *Observability Engineering* (2nd ed.) | Majors, Fong-Jones, Miranda | O'Reilly | Philosophy, SLOs, culture |
| *Distributed Systems Observability* | Cindy Sridharan | O'Reilly (FREE) | Conceptual foundation |
| *Learning OpenTelemetry* | Young, Parker | O'Reilly | OTel fundamentals |
| *Mastering OpenTelemetry and Observability* | Steve Flanders | Wiley | Enterprise adoption |
| *Cloud-Native Observability with OpenTelemetry* | Alex Boten | Packt | Practical implementation |

### Online courses

| Course | Provider | Cost | Duration |
|--------|----------|------|----------|
| Getting Started with OpenTelemetry (LFS148) | Linux Foundation | FREE | 10 hours |
| Observability with Grafana, Prometheus, Loki, Alloy and Tempo | Udemy (Aref Karimi) | ~$20 | Comprehensive |
| Grafana Workshops | Grafana Labs | FREE | Various |
| Prometheus: Complete Hands-On | Udemy | ~$20 | 9.5 hours |

### Official documentation

- OpenTelemetry: https://opentelemetry.io/docs/
- Spring Boot Observability: https://docs.spring.io/spring-boot/reference/actuator/observability.html
- Micrometer: https://micrometer.io/docs
- Prometheus: https://prometheus.io/docs/
- Grafana: https://grafana.com/docs/
- Grafana Loki: https://grafana.com/docs/loki/latest/
- Grafana Tempo: https://grafana.com/docs/tempo/latest/

### GitHub repositories for hands-on practice

| Repository | Description |
|------------|-------------|
| `blueswen/spring-boot-observability` | Complete Spring Boot + LGTM stack with exemplars |
| `mnadeem/boot-opentelemetry-tempo` | Multi-service demo with realistic scenarios |
| `grafana/grafana-opentelemetry-starter` | Official Grafana Spring Boot starter |
| `prometheus-community/helm-charts` | kube-prometheus-stack Helm chart |
| `grafana/docker-otel-lgtm` | Single-container LGTM for local development |

### Conference talks and blog posts

- KubeCon Observability Day sessions (annual, recordings on CNCF YouTube)
- Spring.io blog: "Observability with Spring Boot 3"
- Grafana Labs blog: Production patterns, correlation techniques
- Baeldung: Spring Boot observability tutorials

---

## Curriculum summary by week

| Week | Phase | Focus |
|------|-------|-------|
| 1-2 | Foundations | Three pillars, concepts, context propagation |
| 3-4 | What to Measure | Golden Signals, RED/USE, SLIs/SLOs |
| 5-7 | Instrumentation | Micrometer, OpenTelemetry, logging, correlation |
| 8-10 | Infrastructure | Prometheus, Loki, Tempo, Collector on EKS |
| 11-12 | Accessibility | Dashboards, alerting, runbooks |
| 13-14 | Advanced | High cardinality, sampling, chaos engineering |

### Time commitment

Expect **10-15 hours per week** for optimal progress:
- 3-4 hours reading/courses
- 4-6 hours hands-on implementation
- 2-3 hours milestone project work
- 1-2 hours review and documentation

### Certification pathway

After completing this curriculum, pursue the **OpenTelemetry Certified Associate (OTCA)** certification from CNCF to validate expertise. The LFS148 course completed in Phase 1 provides direct preparation.

---

## Final notes

This curriculum treats observability as a professional discipline requiring both theoretical understanding and extensive hands-on practice. The milestone projects are designed to produce portfolio-quality artifacts demonstrating production-ready capabilities.

The open-source stack (OpenTelemetry + Prometheus + Grafana + Loki + Tempo) represents the industry standard for cloud-native observability. Mastering this stack provides transferable skills applicable across organizations and enables participation in the vibrant CNCF observability ecosystem.

Focus on building mental models: understanding *why* the tools work the way they do enables adapting to new situations. The three pillars complement each other; SLOs drive engineering decisions; correlation enables rapid debugging. These principles remain constant even as specific tools evolve.