---
title: "Observability for Spring Boot on EKS"
category: "Observability"
description: "26-week mastery plan covering metrics, logs, traces, alerting, SRE practices, and infrastructure deployment for Spring Boot on AWS EKS"
---

# Mastery-level observability learning plan for Spring Boot on EKS

**This 26-week plan takes you from basic Prometheus/Grafana familiarity to production-grade observability expertise.** It covers the three pillars — metrics, logs, traces — plus alerting, SRE practices, and infrastructure deployment, all anchored in your Kotlin/Spring Boot and AWS EKS context. Each phase builds on the last, with concrete milestones, the best resources available, and hands-on projects that ensure depth-first mastery rather than surface-level coverage.

The plan is organized into seven phases. Phases 0–1 rebuild your foundations with intentional depth. Phases 2–3 extend into logs and traces. Phases 4–5 layer on SRE practices and production infrastructure. Phase 6 ties everything together in a capstone project that mirrors a real production system. Every phase includes a milestone checkpoint — you should not advance until you can demonstrate the milestone competency.

---

## Phase 0: Conceptual foundations and environment setup (weeks 1–3)

This phase establishes the mental models and local tooling you'll need for everything that follows. Resist the temptation to skip it — the engineers who plateau in observability almost always have gaps in foundational understanding.

### Core reading

Start with **Cindy Sridharan's "Distributed Systems Observability"** (O'Reilly, 2018, ~36 pages, free PDF). This concise ebook frames the three pillars and their trade-offs without vendor bias. Follow it immediately with the first four chapters of **"Observability Engineering" by Charity Majors, Liz Fong-Jones, and George Miranda** (O'Reilly, 2022). This book challenges the traditional monitoring mindset and introduces **structured events, high-cardinality data, and the distinction between observability and monitoring** — concepts that will reshape how you approach every subsequent topic. Read chapters 1–4 now; you'll return to the alerting chapters in Phase 4.

For Prometheus specifically, acquire **"Prometheus: Up & Running" (2nd Edition) by Julien Pivotto and Brian Brazil** (O'Reilly, 2023). This is the definitive reference written by Prometheus core developers. Read chapters 1–5 during this phase to understand the data model, metric types (counters, gauges, histograms, summaries), and the pull-based scraping architecture. You'll work through the remaining chapters in later phases.

### Environment setup

Build a local observability sandbox using **Grafana's `intro-to-mltp` repository** (github.com/grafana/intro-to-mltp). This Docker Compose environment bundles Mimir (metrics), Loki (logs), Tempo (traces), Pyroscope (profiling), and Grafana with pre-provisioned dashboards. It gives you a single-command way to explore the full LGTM stack without Kubernetes complexity. Spend time clicking through every pre-built dashboard to build intuition for what good observability looks like.

Separately, clone **`samber/workshop-prometheus-grafana`** (GitHub) for focused Prometheus/Grafana exercises: Node Exporter setup, PostgreSQL exporter, custom metrics, and PromQL practice.

### Supplementary resources

| Resource | Why it matters |
|----------|---------------|
| OpenTelemetry Observability Primer (opentelemetry.io/docs/concepts/observability-primer) | Establishes spans, traces, metrics, and logs as first-class concepts |
| Google SRE Book, Chapter 6: "Monitoring Distributed Systems" (sre.google, free) | Introduces the **Four Golden Signals** — latency, traffic, errors, saturation |
| Rob Ewaschuk's "My Philosophy on Alerting" (Google doc, widely mirrored) | Foundational alerting principles you'll reference throughout the plan |

### Milestone checkpoint

You can explain the difference between monitoring and observability, articulate why high-cardinality data matters, diagram Prometheus's pull-based architecture, and name the Four Golden Signals with examples from a Spring Boot context. Your local `intro-to-mltp` stack runs, and you can navigate between metrics, logs, and traces in Grafana.

---

## Phase 1: Metrics mastery — Prometheus, PromQL, Grafana, and Micrometer (weeks 4–8)

This is the longest and most critical phase. **PromQL fluency is the single highest-leverage skill** in the entire plan — it underpins dashboards, alerts, recording rules, and SLO implementation.

### PromQL from the ground up (weeks 4–5)

Begin with **PromLabs' self-paced PromQL training** (training.promlabs.com), created by Julius Volz, Prometheus co-founder and original PromQL designer. This is the best PromQL course available at any price. Supplement it with the **PromLabs PromQL cheat sheet** (promlabs.com/promql-cheat-sheet) and Aliaksandr Valialkin's excellent **"PromQL Tutorial for Beginners and Humans"** on Medium.

Key concepts to master with deliberate practice:

- **Instant vectors vs range vectors** — the most common source of PromQL confusion
- **`rate()` vs `irate()`** — `rate()` averages over the range window and is almost always what you want for alerting; `irate()` uses only the last two samples and is useful for volatile, high-resolution dashboards. Read the PromLabs blog post on counter rates for the nuances
- **`histogram_quantile()`** — understand why it works on bucket boundaries, why it interpolates linearly within buckets, and why bucket boundaries matter enormously for accuracy
- **Aggregation operators** (`sum`, `avg`, `topk`, `group`) with `by` and `without` clauses
- **Binary operators and vector matching** — `on()`, `ignoring()`, `group_left()`, `group_right()`
- **Subqueries** — range queries over instant queries, useful for alerting on aggregated data
- **Recording rules** — pre-computed queries that reduce dashboard load and simplify alert expressions

Use **PromLens** (open-source visual query builder from PromLabs) to visualize how PromQL expressions are evaluated. This tool makes vector matching and aggregation immediately comprehensible.

### Prometheus architecture deep dive (week 6)

Now return to "Prometheus: Up & Running" chapters 6–21. Pay particular attention to the storage engine, federation, and remote write/read chapters.

For TSDB internals, the best resource is **Ganesh Vernekar's multi-part blog series** (ganeshvernekar.com/blog/prometheus-tsdb-the-head-block). Vernekar is a Prometheus team member who wrote the definitive technical breakdown: the head block, WAL (write-ahead log), memory-mapped chunks, compaction, and persistent blocks. Complement with **Fabian Reinartz's original TSDB design blog post** ("Writing a Time Series Database from Scratch") and the Grafana Labs blog on TSDB optimizations. Understanding storage internals matters because it directly impacts retention planning, resource sizing, and when you need to scale beyond a single Prometheus instance.

For **scaling Prometheus** beyond a single instance, study the architectural trade-offs between Thanos, Cortex, and Mimir:

| Solution | Architecture | Best for |
|----------|-------------|----------|
| **Thanos** | Sidecar model, uploads to object storage, global query view | Teams already running Prometheus who want long-term storage with minimal architecture changes |
| **Mimir** (Grafana) | Push-based via remote write, horizontally scalable, split-and-merge compactor | High-scale environments needing native multi-tenancy and tested to 1B active series |
| **Cortex** | Push-based, predecessor to Mimir | Legacy; Mimir is its successor. Understand it for context but prefer Mimir for new deployments |

Key comparison resources: the **Grafana Mimir GitHub discussion #3380** provides the team's own comparison, the **Grafana community forum post "Thanos vs Mimir"** offers a practical snapshot table, and the **CECG blog evaluating large-scale multi-tenant metrics** gives a real-world evaluation. The critical insight: **if you're in the Grafana ecosystem (which you are), Mimir is the natural choice**; Thanos is preferable if you want to minimize infrastructure changes and keep a pure pull-based model.

### Spring Boot Actuator and Micrometer (week 7)

This is where observability meets your daily code. **Micrometer is the metrics facade for Spring Boot** — analogous to SLF4J for logging. It abstracts metric creation from the backend, and its Prometheus registry translates everything into Prometheus exposition format.

Start with the **official Micrometer Prometheus registry docs** (docs.micrometer.io), then work through the **DAC.digital tutorial "Custom Metrics with Micrometer and Prometheus Using Kotlin"**, which covers counters, gauges (with GC protection — critical in JVM environments), scheduled metrics, tags, and helper functions specific to Kotlin. Clone **`wojciech-zurek/kotlin-spring-boot-prometheus-grafana-example`** for a complete Docker Compose reference.

Metrics to instrument in every Spring Boot service:

- **JVM metrics** (automatic via Actuator): heap memory, GC pauses, thread count, class loading
- **HTTP request metrics**: `http_server_requests_seconds` histogram — gives you request rate, error rate, and latency percentiles from a single metric
- **Database connection pool**: HikariCP metrics are auto-exported — pool size, active connections, pending threads, connection acquisition time
- **Custom business metrics**: order count, payment processing time, cache hit ratios — these are the metrics that actually tell you if your service is healthy from a user perspective

### Grafana dashboards and visualization (week 8)

Move beyond clicking in the UI to treating dashboards as engineered artifacts. Study the **AndiDog blog post "Grafana Dashboards — Best Practices and Dashboards-as-Code"** for dashboard design principles: high-level business dashboards at the top, drill-down technical dashboards below, consistent color schemes, and the discipline of removing panels that nobody looks at.

Learn **Grafana provisioning** (datasources.yaml, dashboards.yaml in `/etc/grafana/provisioning/`) and **Grafonnet** (github.com/grafana/grafonnet), the official Jsonnet library for programmatic dashboard generation. The **Ceph.io blog on migrating to Grafonnet** reported an **85–90% reduction in dashboard code** compared to raw JSON. Practice with the **Novatec Grafonnet tutorial** for template variables, panels, and rows.

Explore Grafana's alerting system through the **official alerting documentation** (grafana.com/docs/grafana/latest/alerting/) and the GROT Academy alerting course. Understand the difference between Grafana-managed alerts and Prometheus-managed alerts (via Alertmanager) — you'll use both, but Alertmanager is the production standard for Prometheus-native alerting.

### Kubernetes-level metrics

Understand the three sources of Kubernetes metrics and what each provides:

- **kube-state-metrics**: Kubernetes object states — deployment replicas desired vs available, pod phases, node conditions. Think of it as "what Kubernetes knows about your workloads"
- **node-exporter**: Host-level hardware and OS metrics — CPU, memory, disk I/O, network. Think of it as "what the operating system knows"
- **cAdvisor** (built into kubelet): Container-level resource usage — CPU/memory per container, network I/O per pod. Think of it as "what the container runtime knows"

The **kube-prometheus-stack Helm chart** installs all three automatically with pre-built dashboards and alerting rules. Study the **Spacelift guide on kube-state-metrics** for practical configuration.

### Alertmanager

Read the **Prometheus Alertmanager official configuration docs** alongside the **Grafana Labs step-by-step guide for Slack, PagerDuty, and Gmail integration**. The **Sysdig Alertmanager best practices** post covers routing trees, grouping (group_by, group_wait, group_interval), inhibition (where critical alerts suppress warning-level duplicates), and organizational scaling patterns. Use the **Prometheus routing tree visualization tool** (prometheus.io/webtools/alerting/routing-tree-editor/) to test complex routing configurations before deploying them.

### AWS CloudWatch integration

For monitoring AWS-managed services (RDS, Aurora, ALB, etc.) with Prometheus, use **YACE — Yet Another CloudWatch Exporter** (github.com/prometheus-community/yet-another-cloudwatch-exporter). As of November 2024, YACE joined the official Prometheus community. It auto-discovers AWS resources via tags, supports 24+ services, and batches API calls efficiently (500 metrics per request). For database-specific monitoring, **Qonto's Prometheus RDS Exporter** provides 30 pre-built alerts with runbooks and a Helm chart for EKS.

### Custom Prometheus exporters

For cases where no existing exporter covers your needs, study the **Prometheus "Writing Exporters" guide** (prometheus.io/docs/instrumenting/writing_exporters/). For JVM services, Micrometer handles most use cases, but for sidecar services or external systems, the **Civo tutorial "Build Your Own Prometheus Exporter in Go"** walks through creating a ServiceMonitor-compatible exporter with counters and histograms.

### Milestone checkpoint

You can write PromQL queries that calculate error rates using `rate()`, compute p99 latency from histograms using `histogram_quantile()`, and aggregate across dimensions using `sum by`. You have a Spring Boot application exposing custom Micrometer metrics scraped by Prometheus via a ServiceMonitor. Your Grafana dashboard shows JVM, HTTP, and business metrics with template variables for service selection. Alertmanager routes a test alert to Slack.

---

## Phase 2: Logging — structured logs, Loki, and log aggregation (weeks 9–11)

### Structured logging for Spring Boot (week 9)

Stop logging unstructured strings. **Structured JSON logging is non-negotiable for production observability** — it enables machine parsing, label extraction in Loki, and correlation with traces.

The standard library is **logstash-logback-encoder** (github.com/logfellow/logstash-logback-encoder, v9.0+). It integrates with SLF4J/Logback to output JSON with automatic MDC (Mapped Diagnostic Context) field inclusion. Configure it to always include `traceId`, `spanId`, `serviceName`, `environment`, and any request-scoped context (user ID, request ID) in MDC.

Key resources: the **SigNoz "Spring Boot Logging Complete Guide"** covers Logback configuration, JSON output, MDC for trace correlation, and async appenders. The **Asimio Tech blog** (tech.asimio.net) provides a focused walkthrough with a GitHub repo. Note that **Spring Boot 3.4+ added native structured logging support**, but logstash-logback-encoder remains more feature-rich for production use.

### Grafana Loki and LogQL (week 10)

Loki's architecture is fundamentally different from Elasticsearch: **it indexes only labels (metadata), not log content**. This makes it dramatically cheaper to operate but means query performance depends heavily on label design. You filter by labels first, then grep within the matching log streams.

Work through the **official Loki documentation** (grafana.com/docs/loki/latest/) focusing on architecture and deployment modes (monolithic for dev, microservices for production). The **Loki quickstart tutorial** with the Carnivorous Greenhouse demo app provides interactive practice.

For **LogQL mastery**, study the official query reference, then work through **Helge Klein's "LogQL: A Primer"** for an accessible introduction comparing LogQL concepts to Splunk. Key patterns to master:

- **Stream selectors**: `{namespace="production", app="order-service"}`
- **Filter expressions**: `|= "error"`, `!~ "health_check"`
- **Parsers**: `| json` for structured JSON logs, `| logfmt`, `| regexp`
- **Metric queries**: `rate({app="order-service"} |= "error" [5m])` — turns log lines into time-series metrics
- **Unwrap expressions**: extract numeric values from log fields for aggregation

**When NOT to use Loki**: If your primary use case is full-text forensic search across massive log volumes with complex boolean queries (security/compliance analysis), Elasticsearch is the stronger tool. Loki excels at label-driven exploration and is **typically 3–10x cheaper to operate** on Kubernetes because it uses object storage (S3) rather than expensive indexed storage. For most application observability, Loki wins on cost and operational simplicity.

### Log aggregation patterns in Kubernetes (week 11)

| Pattern | How it works | Best for |
|---------|-------------|----------|
| **DaemonSet** (one agent per node) | Fluent Bit or Grafana Alloy reads container logs from `/var/log/containers/` on each node | Standard stdout/stderr logging — the default choice for 90% of cases |
| **Sidecar** (one agent per pod) | Agent container runs alongside your app container, reads from shared volume | Apps that write logs to files, multi-format logging, strict per-app isolation |

The **Fluent Bit official blog "Common Architecture Patterns"** (fluentbit.io/blog) is the authoritative breakdown. For Loki specifically, **Grafana Alloy** (formerly Promtail) is the recommended collection agent. Read the **IsItObservable tutorial** for a practical Fluent Bit DaemonSet configuration.

### Milestone checkpoint

Your Spring Boot application outputs structured JSON logs with traceId/spanId in every line. Loki ingests these logs via Alloy/Promtail in a DaemonSet pattern. You can write LogQL queries that filter by label, parse JSON fields, and calculate error rates as metrics. You can navigate from a log line in Grafana to the corresponding trace (via derived fields configuration).

---

## Phase 3: Distributed tracing — OpenTelemetry, Tempo, and correlation (weeks 12–15)

### OpenTelemetry concepts and Java instrumentation (weeks 12–13)

**OpenTelemetry (OTel) is the industry standard** for telemetry collection. It's a CNCF project that has unified the previously fragmented OpenTracing and OpenCensus efforts. Every major observability vendor supports it.

Start with the **OpenTelemetry Observability Primer** (opentelemetry.io/docs/concepts/observability-primer/) to solidify concepts: **spans** (units of work with timing, attributes, and parent-child relationships), **traces** (DAGs of spans representing end-to-end requests), **context propagation** (how trace context flows across service boundaries via HTTP headers or gRPC metadata), and **baggage** (key-value pairs propagated across services — use sparingly).

For instrumentation, you have three approaches in order of increasing control:

1. **Java agent (zero-code)**: Attach `opentelemetry-javaagent.jar` via `-javaagent` flag. Automatically instruments 150+ libraries including Spring MVC, WebFlux, JDBC, JPA, Kafka, and HTTP clients. **Start here** — it provides comprehensive coverage with zero code changes
2. **Spring Boot starter** (`opentelemetry-spring-boot-starter`): Spring-native configuration via `application.yml`. Better for GraalVM native images and when you need fine-grained control without the agent overhead. The starter **reached stable status in 2024**
3. **Manual instrumentation**: Use the OpenTelemetry SDK to create custom spans, add attributes, and record events. Essential for business-critical paths where you need domain-specific span data

The **OpenTelemetry Getting Started for Java** tutorial walks through the "roll a dice" Spring Boot example. For production patterns, study the **opentelemetry-java-examples GitHub repo** (official examples including agent, starter, Micrometer bridge, and log appenders). The **Grafana OpenTelemetry Starter** (github.com/grafana/grafana-opentelemetry-starter) provides a batteries-included Spring Boot starter optimized for the Grafana stack.

The **Micrometer-to-OpenTelemetry bridge** (`micrometer-tracing-bridge-otel`) is important: it lets Spring Boot Actuator's built-in tracing integrate with OpenTelemetry, meaning your `@Timed` annotations and HTTP trace IDs flow through the OTel pipeline. This bridge is the recommended approach when you want Micrometer metrics and OTel traces to coexist.

**Book**: Read **"Mastering Distributed Tracing" by Yuri Shkuro** (Packt, 2019) — written by the creator of Jaeger. It covers theory, instrumentation patterns, and infrastructure operation with depth that no tutorial matches. Complement with **"Distributed Tracing in Practice" by Austin Parker et al.** (O'Reilly, 2020) for a more vendor-neutral perspective.

### Trace backends: Jaeger vs Grafana Tempo (week 14)

| Dimension | Jaeger | Grafana Tempo |
|-----------|--------|---------------|
| **Storage** | Database-backed (Elasticsearch, Cassandra, ClickHouse) | Object storage (S3, GCS) — dramatically cheaper |
| **Querying** | Rich attribute-based search in dedicated UI | Trace ID lookup + TraceQL in Grafana |
| **UI** | Built-in, purpose-built trace visualization | Grafana (shared with metrics and logs) |
| **Operational cost** | Higher — requires database cluster management | Lower — no indexing, minimal compute |
| **Metrics from traces** | Not built-in | Built-in metrics-generator produces RED metrics from spans |
| **Ecosystem** | Standalone, CNCF Graduated | Deep Grafana integration (exemplars, derived fields, unified dashboards) |

**Recommendation for your stack**: Since you're already invested in Grafana, **Tempo is the natural choice**. It integrates natively with Grafana for trace-to-log and trace-to-metric correlation, uses cheap S3 storage, and its metrics-generator eliminates the need to separately compute RED metrics. Understand Jaeger's strengths (powerful search, dedicated UI) so you know when it's the better tool — particularly for teams that need ad-hoc trace analysis without Grafana.

Note: **Jaeger v2 (released November 2024) is built on the OpenTelemetry Collector**, making it architecturally closer to Tempo's collection model. Jaeger v1 was deprecated in January 2026.

### Sampling strategies and correlation (week 15)

At production scale, you cannot store every trace. Sampling strategies determine which traces to keep:

- **Head-based sampling**: Decision made at trace creation (first span). Simple, low overhead, but can't make decisions based on trace outcome. Use `TraceIDRatioBased` sampler for probabilistic sampling. Good for: consistent baseline coverage
- **Tail-based sampling**: Decision made after the trace completes. Can sample based on latency, error status, or specific attributes. Requires an OTel Collector with the `tailsamplingprocessor`. More complex operationally — the collector must buffer complete traces before deciding. Good for: always capturing errors and slow requests

The **OpenTelemetry sampling documentation** (opentelemetry.io/docs/concepts/sampling/) is the conceptual reference. **Goutham Veeramachaneni's "Sampling at scale with OpenTelemetry"** (gouthamve.dev) shares real production experience at Grafana Labs handling 2M spans/second. The **OpenTelemetry Collector tail sampling processor README** (GitHub) is the configuration reference.

**Practical recommendation**: Start with head-based probabilistic sampling (e.g., 10% of traces) plus always-on sampling for errors. Add tail-based sampling as your volume grows and you need more sophisticated policies.

For **signal correlation** — the ability to jump between metrics, logs, and traces for the same request — study:

- **Exemplars**: Prometheus metadata that links a metric sample to a specific trace ID. Visualized as diamond dots on Grafana charts. Read the **Grafana exemplars documentation** and **Bartek Plotka's deep dive "Correlating Signals Efficiently in Modern Observability"** (bwplotka.dev)
- **Derived fields in Loki**: Configure Loki data source to extract traceId from log lines and link to Tempo
- **Tempo metrics-generator**: Automatically produces service graphs and RED metrics with exemplars, creating bidirectional trace↔metric links

### Hands-on projects for this phase

Deploy the **OpenTelemetry Demo (Astronomy Shop)** — a 15+ microservice application in multiple languages that includes a **Kotlin Fraud Detection Service**. It ships with Docker and Kubernetes deployment manifests, Grafana dashboards, and feature flag scenarios for troubleshooting practice. Also explore **`alexschroth/spring-boot-demo-otel-manual`** for a focused manual instrumentation demo with Prometheus, Loki, and Tempo via Docker Compose.

### Milestone checkpoint

Your Spring Boot services produce traces via the OTel Java agent. Traces flow through an OTel Collector to Tempo. In Grafana, you can click from a latency spike on a dashboard (metric) → the exemplar trace → individual spans → the correlated log lines in Loki. You understand when to use head-based vs tail-based sampling and can configure both.

---

## Phase 4: SRE practices — SLOs, error budgets, and alerting philosophy (weeks 16–19)

### SLIs, SLOs, and error budgets (weeks 16–17)

This phase shifts from "how do I collect data" to "how do I use data to make reliability decisions." The core reading is **three resources that build on each other**:

1. **Google SRE Book, Chapter 4: "Service Level Objectives"** (sre.google, free) — establishes the conceptual framework for SLIs, SLOs, and SLAs
2. **Google SRE Workbook, Chapter 2: "Implementing SLOs"** (sre.google, free) — provides practical tables mapping service types to appropriate SLIs (availability SLI for request-driven services, freshness SLI for data pipelines, etc.)
3. **"Implementing Service Level Objectives" by Alex Hidalgo** (O'Reilly, 2020) — the definitive deep dive. Hidalgo (ex-Google SRE) covers mathematical models, statistical techniques, organizational buy-in, and the common failure modes of SLO programs. **This book is essential.**

Key concepts: an **SLI** (Service Level Indicator) is a quantitative measure of service behavior (e.g., proportion of requests completing in under 300ms). An **SLO** (Service Level Objective) is a target value for an SLI (e.g., 99.9% of requests under 300ms over 30 days). The **error budget** is `1 - SLO target` — for 99.9%, you have a 0.1% error budget, meaning you can "afford" 43 minutes of downtime per month.

For practical implementation, study **Sloth** (sloth.dev) — a Prometheus SLO generator that takes YAML SLO definitions and auto-generates multi-window multi-burn-rate alert rules plus Grafana dashboards. Also evaluate **Pyrra** (github.com/pyrra-dev/pyrra), which adds a web UI for SLO management. The **David Calvert blog comparing Sloth vs Pyrra** (0xdc.me) provides a balanced evaluation. The **OpenSLO specification** (openslo.com) offers a vendor-neutral YAML standard for defining SLOs as code.

### Burn-rate alerting (week 18)

This is the most important alerting technique in the plan. **Multi-window multi-burn-rate alerting** is the recommended approach from Google's SRE Workbook (Chapter 5: "Alerting on SLOs"), and it solves the fundamental tension between alert speed and alert noise.

The SRE Workbook Chapter 5 walks through **six progressively better alerting approaches**, from naive threshold alerts (too noisy) to multi-window burn-rate alerts (optimal). The key insight: instead of alerting when error rate exceeds a static threshold, **alert when the rate of error budget consumption (burn rate) exceeds a level that would exhaust the budget before the SLO window ends**. Fast-burn alerts (e.g., 14.4x burn rate over 1 hour, confirmed by 5-minute window) page immediately. Slow-burn alerts (e.g., 3x burn rate over 3 days, confirmed by 6-hour window) create tickets.

Study the **SoundCloud engineering post "Alerting on SLOs like Pros"** for a real-world implementation with PromQL recording rules and practical configuration decisions. The **Mattermost blog on Sloth SLO implementation** shows production use with Thanos.

### Alert design philosophy (week 18, continued)

Read **Rob Ewaschuk's "My Philosophy on Alerting"** carefully — it's the foundational document that influenced the entire Google SRE alerting approach. Core principles: **every page must be urgent, important, actionable, and real**. Alert on symptoms (user-facing impact) not causes (CPU is high). Use tickets for non-urgent issues. Include runbooks with every alert.

The **Prometheus project's official alerting best practices** page (prometheus.io/docs/practices/alerting/) explicitly references Ewaschuk's philosophy. The **CASE Method by Cory Watson** (onemogin.com) provides a memorable framework: alerts should be Context-rich, Actionable, Symptom-based, and Evaluated.

Return to **"Observability Engineering" chapters 14–17** for Charity Majors' perspective on why traditional monitoring creates dangerous alert fatigue and how SLO-based alerting resolves it.

### Incident response and on-call (week 19)

Read the **Google SRE Book chapters 11 (Being On-Call), 14 (Managing Incidents), and 15 (Postmortem Culture)**. Then study **PagerDuty's open-source Incident Response Guide** (response.pagerduty.com) — it's the most practical, battle-tested IR documentation publicly available, covering severity levels, incident commander roles, communication protocols, and after-incident processes. Their **Postmortem Guide** (postmortems.pagerduty.com) includes ready-to-use templates.

For deeper understanding of failure in complex systems, read **Richard Cook's "How Complex Systems Fail"** (~4 pages, freely available) and consider **Sidney Dekker's "The Field Guide to Understanding 'Human Error'"** for building genuine blameless culture.

For incident management tooling, the market has shifted significantly. **PagerDuty** remains the alerting/mobilization leader (~70% Fortune 100), but modern tools like **incident.io** (Slack-native with AI-assisted response) and **Rootly** (no-code automation, comprehensive retrospectives) focus more on the full incident lifecycle including organizational learning. **Grafana OnCall** (managed Cloud version) integrates natively with your Grafana stack for simpler use cases.

### Milestone checkpoint

You can define SLIs and SLOs for a Spring Boot service (availability and latency), implement them using Sloth with Prometheus, and explain multi-window multi-burn-rate alerting with concrete PromQL examples. You can articulate why symptom-based alerting reduces alert fatigue and write a blameless postmortem following the PagerDuty template.

---

## Phase 5: Production infrastructure — deployment, IaC, and GitOps (weeks 20–23)

### Deploying the observability stack on EKS (weeks 20–21)

The **kube-prometheus-stack Helm chart** (github.com/prometheus-community/helm-charts) is your starting point. It installs the Prometheus Operator, Prometheus, Alertmanager, Grafana, node-exporter, and kube-state-metrics with pre-built dashboards and alert rules. Follow the **BootVar comprehensive setup guide** or **DevOpsCube Prometheus Helm guide** (includes resource calculation examples for real projects).

Master the **Prometheus Operator CRDs** — these are how you declaratively configure monitoring in Kubernetes:

- **ServiceMonitor**: Tells Prometheus to scrape a Kubernetes Service. Create one for each Spring Boot app pointing at the `/actuator/prometheus` endpoint
- **PodMonitor**: Scrapes pods directly — useful for jobs, sidecars, or services without a ClusterIP
- **PrometheusRule**: Defines alerting and recording rules as Kubernetes resources
- **AlertmanagerConfig**: Configures routing and receivers per-namespace
- **ScrapeConfig**: Monitors external targets outside the cluster

The **Prometheus Operator getting started guide** (prometheus-operator.dev) and **Container Solutions beginner guide** provide clear CRD walkthroughs.

Deploy **Loki in microservices mode** with S3 backend using the official Helm chart (grafana.com/docs/loki/latest/setup/install/helm/). The simple scalable deployment mode is being deprecated; microservices mode is the production standard. Deploy **Tempo** using the `tempo-distributed` Helm chart with S3 backend.

For the **OpenTelemetry Operator**, follow the **official OTel Kubernetes Getting Started guide** and the **KubeCon EU 2024 tutorial** (github.com/pavolloffay/kubecon-eu-2024-opentelemetry-kubernetes-tracing-tutorial) for hands-on auto-instrumentation. Deploy collectors in a **DaemonSet + Gateway pattern**: DaemonSets on each node handle collection, a centralized Deployment handles processing, batching, and export.

### Infrastructure as Code with Terraform (week 22)

Use the **Grafana Terraform provider** (registry.terraform.io/providers/grafana/grafana) to manage dashboards, data sources, folders, alert rules, notification policies, and contact points as code. The **Grafana docs "Dashboards with Terraform + GitHub Actions"** tutorial shows a complete CI/CD pipeline: PRs show `terraform plan` diffs, merge triggers `terraform apply` to sync dashboard JSON to Grafana.

For the broader observability infrastructure, use Terraform for:

- EKS cluster and node groups (you already know this)
- Helm releases for kube-prometheus-stack, Loki, Tempo (via the `helm_release` resource)
- Grafana dashboards and alert rules (via the Grafana provider)
- AWS resources like S3 buckets for Loki/Tempo storage, IAM roles for IRSA

Store **PrometheusRule CRDs and ServiceMonitor CRDs as YAML in Git**. These are Kubernetes-native and deploy through your standard GitOps pipeline rather than Terraform.

### GitOps for monitoring configuration (week 22, continued)

Wrap your Helm releases in **ArgoCD Application custom resources**. The **Medium walkthrough "Deploying Prometheus and Grafana to Kubernetes the GitOps Way Using ArgoCD"** provides an EKS-specific guide. Key practice: use the **app-of-apps pattern** to manage monitoring stack components (kube-prometheus-stack, Loki, Tempo, OTel Operator) as a single declarative unit. Use **sync waves** to handle CRD ordering (CRDs must deploy before ServiceMonitors/PrometheusRules that reference them).

### Cost optimization (week 23)

Observability costs grow faster than application traffic because each new service, endpoint, and label multiplies your metric cardinality. **Cardinality management is the single most impactful cost lever**.

Key strategies from the **Grafana Labs blog "How to Manage High Cardinality Metrics"** and **Cloud Native Now's "Cost-Aware Observability on K8s"**:

- **Tiered scrape intervals**: 10s for critical business metrics, 30s for infrastructure, 60s for low-priority targets
- **Drop unused labels** via `metric_relabel_configs` in Prometheus — labels like `pod` on aggregated metrics waste storage
- **Recording rules** for frequently-queried aggregations — pre-compute once rather than computing on every dashboard load
- **Loki label discipline**: Index only low-cardinality labels (namespace, app, level). Never index request IDs or user IDs as labels
- **Trace sampling**: Even 10% head-based sampling captures enough data for most troubleshooting while reducing storage 90%
- **Retention policies**: 15-day high-resolution metrics, 13-month downsampled data (via Thanos/Mimir), 30-day log retention, 7-day trace retention

### Milestone checkpoint

Your full LGTM stack runs on EKS deployed via ArgoCD. Grafana dashboards and alert rules are managed by Terraform with CI/CD. ServiceMonitors and PrometheusRules live in Git alongside application code. You can articulate the cost implications of adding a new label to a high-cardinality metric and implement mitigation strategies.

---

## Phase 6: Capstone project and certification (weeks 24–26)

### Capstone project: production observability platform

Build an end-to-end observability platform that demonstrates every skill from the plan:

1. **Infrastructure**: EKS cluster provisioned with Terraform, IRSA for pod-level AWS access
2. **Observability stack**: kube-prometheus-stack + Loki (microservices, S3) + Tempo (S3) + OTel Operator, all deployed via ArgoCD
3. **Application**: 3–5 Kotlin/Spring Boot microservices with Micrometer metrics, structured JSON logging, and OTel auto-instrumentation
4. **Database monitoring**: RDS/Aurora monitored via YACE CloudWatch Exporter and Qonto RDS Exporter
5. **Dashboards as code**: Grafonnet dashboards in Git, deployed via Terraform Grafana provider with GitHub Actions CI/CD
6. **SLOs**: Availability and latency SLOs defined in Sloth, with multi-window burn-rate alerts routing to Slack (warning) and PagerDuty (critical)
7. **Correlation**: Exemplars linking Prometheus metrics to Tempo traces, derived fields linking Loki logs to traces
8. **Cost controls**: Tiered scrape intervals, metric relabeling, head-based trace sampling, 15-day metric retention with Mimir for long-term storage

### Certification preparation

The **Prometheus Certified Associate (PCA)** from CNCF/Linux Foundation validates your Prometheus expertise. The exam covers: observability concepts (18%), Prometheus fundamentals (20%), **PromQL (28%)** — the largest domain, instrumentation and exporters (16%), dashboarding and alerting (18%). The **LFS241 course** (Linux Foundation, $299 bundled with exam) provides structured preparation. Supplement with the **`walidshaari/PrometheusCertifiedAssociate` GitHub repo** for study guides and practice resources.

### Continuing education

| Resource | Purpose |
|----------|---------|
| **Grafana GROT Academy** (learn.grafana.com) | Free self-paced courses with hands-on labs, badges, and certifications for the full LGTM stack |
| **Grafana Workshops** (grafana.com/workshops) | Free hands-on workshops on Alloy pipelines, application monitoring, OpenTelemetry |
| **Grafana Killercoda tutorials** (killercoda.com/grafana-labs) | Interactive browser-based labs for Loki, Alloy, Prometheus |
| **PromCon** (promcon.io) and **GrafanaCON** recordings | Annual conference talks — watch TSDB deep dives, Mimir scaling talks, and community presentations |
| **SREcon** (usenix.org) | USENIX's SRE conference — talks on incident analysis, SLO adoption at scale, and chaos engineering |
| **o11ycast podcast** | Hosted by Charity Majors and Liz Fong-Jones — ongoing observability thought leadership |
| **Robust Perception blog** (robustperception.io) | Brian Brazil's Prometheus best practices |
| **PromLabs blog and YouTube** (promlabs.com/blog) | Julius Volz's ongoing PromQL deep dives and Prometheus release analysis |

---

## Complete book list, ordered by when to read them

| Phase | Book | Author(s) | Publisher/Year |
|-------|------|-----------|---------------|
| 0 | *Distributed Systems Observability* (free) | Cindy Sridharan | O'Reilly, 2018 |
| 0–1 | *Observability Engineering* | Charity Majors, Liz Fong-Jones, George Miranda | O'Reilly, 2022 |
| 1 | *Prometheus: Up & Running* (2nd ed.) | Julien Pivotto, Brian Brazil | O'Reilly, 2023 |
| 3 | *Mastering Distributed Tracing* | Yuri Shkuro | Packt, 2019 |
| 3 | *Distributed Tracing in Practice* | Austin Parker et al. | O'Reilly, 2020 |
| 3 | *Cloud Observability in Action* | Michael Hausenblas | Manning, 2024 |
| 4 | *Site Reliability Engineering* (free) | Betsy Beyer et al. | O'Reilly/Google, 2016 |
| 4 | *The Site Reliability Workbook* (free) | Betsy Beyer et al. | O'Reilly/Google, 2018 |
| 4 | *Implementing Service Level Objectives* | Alex Hidalgo | O'Reilly, 2020 |
| 4 | *Accelerate* | Nicole Forsgren, Jez Humble, Gene Kim | IT Revolution, 2018 |
| 5 | *Observability with Grafana* | Packt, ~2024 | Covers LGTM stack deployment |

---

## Conclusion

The highest-leverage investments in this plan are **PromQL fluency** (it unlocks dashboards, alerts, recording rules, and SLO implementation), **structured logging with trace correlation** (it eliminates the "grep and pray" debugging antipattern), and **SLO-based alerting** (it replaces noisy threshold alerts with principled reliability measurement). These three capabilities compound: PromQL enables SLO measurement, trace IDs connect logs to metrics, and SLOs give you the decision framework to know whether your system is actually reliable — not just whether individual metrics look normal.

The plan intentionally avoids the common trap of installing every tool before understanding any of them. Each phase produces a working system you can use in production while building the conceptual foundation for the next phase. The architectural trade-off discussions (Thanos vs Mimir, Loki vs Elasticsearch, Jaeger vs Tempo, head vs tail sampling) are not academic — they're the decisions that separate engineers who configure monitoring from engineers who design observability systems.